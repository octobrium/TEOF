# wipe old results
rm -f datasets/reference/results/*

# sanity-check one file
python3 extensions/validator/teof_ocers_min.py datasets/reference/inputs/006_reuters_article.txt datasets/reference/results

# inspect JSON
sed -n '1,80p' datasets/reference/results/006_reuters_article.json

# run full batch + aggregate
bash scripts/run_reference_set.sh
python3 scripts/aggregate_reference_results.py

# peek at master
sed -n '1,60p' datasets/reference/reference_report.md
#!/usr/bin/env python3
"""
TEOF OCERS Minimal Heuristic Validator (v0.2)
- Broader keyword coverage
- Unicode/smart-quote normalization
- Lowercased analysis
- More robust signals for each pillar
"""

import sys, json, re, hashlib, datetime, os, pathlib, unicodedata

def read_text(path):
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

def norm_text(t: str) -> str:
    # Normalize unicode (smart quotes, dashes), collapse whitespace, lowercase
    t = unicodedata.normalize("NFKC", t)
    t = re.sub(r"\s+", " ", t)
    return t.lower().strip()

def count_patterns(t: str, patterns):
    return sum(len(re.findall(p, t, re.I)) for p in patterns)

def any_pattern(t: str, patterns):
    return any(re.search(p, t, re.I) for p in patterns)

# ---------- Scorers ----------

def score_observation(t: str) -> int:
    # Signals that the author anchors claims in observation/uncertainty/risk
    uncertainty = [
        r"\buncertain|uncertainty|unknown|assumption|assume|approximate|estimate\b",
        r"\bmay\b", r"\bmight\b", r"\bcould\b", r"\bwe recognize\b", r"\bwe acknowledge\b",
        r"\brisk(s)?\b", r"\blimitation(s)?\b", r"\bevidence\b", r"\bobserv(a|e)tion|observe|measurement|perception\b"
    ]
    hits = count_patterns(t, uncertainty)
    # Dogmatic language reduces O slightly
    dogma = count_patterns(t, [r"\bguaranteed\b", r"\bundeniable\b", r"\bwithout question\b", r"\bno doubt\b"])
    s = 1 + min(3, hits) - min(1, dogma > 0)
    # Cap/clip
    return max(1, min(5, s + (1 if "data" in t or "evidence" in t else 0)))

def score_coherence(t: str) -> int:
    # Presence of structure markers vs. contradictions
    structure = count_patterns(t, [
        r"\btherefore\b", r"\bthus\b", r"\bhence\b", r"\bbecause\b", r"\bconsequently\b",
        r"\bfirst(ly)?\b", r"\bsecond(ly)?\b", r"\bconclude(s|d)?\b", r"\bimplies?\b",
        r"\bpremise\b"
    ])
    enumeration = count_patterns(t, [r"\b(1\.)\b", r"\b(2\.)\b", r"\b(3\.)\b", r"•", r" - "])
    contradictions = count_patterns(t, [r"\bcontradict(s|ion|ory)?\b", r"\binconsistent\b"])
    hedging_only = count_patterns(t, [r"\bhowever\b", r"\bbut\b"])  # many 'however' without structure can be messy
    s = 2 + min(2, structure) + min(1, enumeration > 1) - min(2, contradictions) - min(1, hedging_only > 3)
    return max(1, min(5, s))

def score_ethics(t: str) -> int:
    # Ethics in TEOF: transparency, clarity, non-manipulation, societal benefit
    positive = count_patterns(t, [
        r"\btransparen(cy|t)\b", r"\baccountab(le|ility)\b", r"\bdisclos(e|ure)\b",
        r"\bfair(ness)?\b", r"\bbias testing\b", r"\baudit(s|ing)?\b", r"\bpublic\b",
        r"\bfor the (public|community|society)\b", r"\bopen(-|\s)?source\b", r"\bresponsible\b",
        r"\bethical\b", r"\bcoheren\w*\b", r"\btruth\b"
    ])
    negative = count_patterns(t, [
        r"\bmanipulat(e|ive|ion)\b", r"\bdeceiv(e|ing|ed|es)\b", r"\bobfuscat(e|ion)\b",
        r"\bpropaganda\b", r"\bthey don'?t want you to know\b", r"\bdon'?t trust (government|media)s?\b",
        r"\bguaranteed returns?\b", r"\bget rich\b", r"\bfear\b"
    ])
    s = 2 + min(3, positive) - min(2, negative)
    return max(1, min(5, s))

def score_repro(t: str) -> int:
    # Reproducibility: methods, numbers, citations, datasets, code
    methods = count_patterns(t, [
        r"\bmethod(s)?\b", r"\bprocedure(s)?\b", r"\bprotocol\b", r"\bchecklist\b",
        r"\balgorithm(s)?\b", r"\bmodel(s)?\b", r"\bexperiment(s|al)?\b"
    ])
    citations = count_patterns(t, [r"\bsource(s)?\b", r"\bcite|citation|reference(s)?\b", r"\bdoi:?\b", r"http[s]?://"])
    numbers = len(re.findall(r"\b\d+(\.\d+)?%?\b", t))  # presence of stats/figures
    code_data = count_patterns(t, [r"\bcode\b", r"\breleased?\b", r"\bgithub\b", r"\bdataset(s)?\b", r"\bappendix\b"])
    s = 1 + min(2, methods > 0) + min(2, citations > 0) + min(1, numbers > 2) + min(1, code_data > 0)
    return max(1, min(5, s))

def score_selfrepair(t: str) -> int:
    # Self-Repair: audits, monitoring, incident reporting, rollback/pauses, indepen

cat > extensions/validator/teof_ocers_min.py <<'EOF'
#!/usr/bin/env python3
"""
TEOF OCERS Minimal Heuristic Validator (v0.2)
- Broader keyword coverage
- Unicode/smart-quote normalization
- Lowercased analysis
- More robust signals for each pillar
"""

import sys, json, re, hashlib, datetime, os, pathlib, unicodedata

def read_text(path):
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

def norm_text(t: str) -> str:
    # Normalize unicode (smart quotes, dashes), collapse whitespace, lowercase
    t = unicodedata.normalize("NFKC", t)
    t = re.sub(r"\s+", " ", t)
    return t.lower().strip()

def count_patterns(t: str, patterns):
    return sum(len(re.findall(p, t, re.I)) for p in patterns)

def any_pattern(t: str, patterns):
    return any(re.search(p, t, re.I) for p in patterns)

# ---------- Scorers ----------

def score_observation(t: str) -> int:
    # Signals that the author anchors claims in observation/uncertainty/risk
    uncertainty = [
        r"\buncertain|uncertainty|unknown|assumption|assume|approximate|estimate\b",
        r"\bmay\b", r"\bmight\b", r"\bcould\b", r"\bwe recognize\b", r"\bwe acknowledge\b",
        r"\brisk(s)?\b", r"\blimitation(s)?\b", r"\bevidence\b", r"\bobserv(a|e)tion|observe|measurement|perception\b"
    ]
    hits = count_patterns(t, uncertainty)
    # Dogmatic language reduces O slightly
    dogma = count_patterns(t, [r"\bguaranteed\b", r"\bundeniable\b", r"\bwithout question\b", r"\bno doubt\b"])
    s = 1 + min(3, hits) - min(1, dogma > 0)
    # Cap/clip
    return max(1, min(5, s + (1 if "data" in t or "evidence" in t else 0)))

def score_coherence(t: str) -> int:
    # Presence of structure markers vs. contradictions
    structure = count_patterns(t, [
        r"\btherefore\b", r"\bthus\b", r"\bhence\b", r"\bbecause\b", r"\bconsequently\b",
        r"\bfirst(ly)?\b", r"\bsecond(ly)?\b", r"\bconclude(s|d)?\b", r"\bimplies?\b",
        r"\bpremise\b"
    ])
    enumeration = count_patterns(t, [r"\b(1\.)\b", r"\b(2\.)\b", r"\b(3\.)\b", r"•", r" - "])
    contradictions = count_patterns(t, [r"\bcontradict(s|ion|ory)?\b", r"\binconsistent\b"])
    hedging_only = count_patterns(t, [r"\bhowever\b", r"\bbut\b"])  # many 'however' without structure can be messy
    s = 2 + min(2, structure) + min(1, enumeration > 1) - min(2, contradictions) - min(1, hedging_only > 3)
    return max(1, min(5, s))

def score_ethics(t: str) -> int:
    # Ethics in TEOF: transparency, clarity, non-manipulation, societal benefit
    positive = count_patterns(t, [
        r"\btransparen(cy|t)\b", r"\baccountab(le|ility)\b", r"\bdisclos(e|ure)\b",
        r"\bfair(ness)?\b", r"\bbias testing\b", r"\baudit(s|ing)?\b", r"\bpublic\b",
        r"\bfor the (public|community|society)\b", r"\bopen(-|\s)?source\b", r"\bresponsible\b",
        r"\bethical\b", r"\bcoheren\w*\b", r"\btruth\b"
    ])
    negative = count_patterns(t, [
        r"\bmanipulat(e|ive|ion)\b", r"\bdeceiv(e|ing|ed|es)\b", r"\bobfuscat(e|ion)\b",
        r"\bpropaganda\b", r"\bthey don'?t want you to know\b", r"\bdon'?t trust (government|media)s?\b",
        r"\bguaranteed returns?\b", r"\bget rich\b", r"\bfear\b"
    ])
    s = 2 + min(3, positive) - min(2, negative)
    return max(1, min(5, s))

def score_repro(t: str) -> int:
    # Reproducibility: methods, numbers, citations, datasets, code
    methods = count_patterns(t, [
        r"\bmethod(s)?\b", r"\bprocedure(s)?\b", r"\bprotocol\b", r"\bchecklist\b",
        r"\balgorithm(s)?\b", r"\bmodel(s)?\b", r"\bexperiment(s|al)?\b"
    ])
    citations = count_patterns(t, [r"\bsource(s)?\b", r"\bcite|citation|reference(s)?\b", r"\bdoi:?\b", r"http[s]?://"])
    numbers = len(re.findall(r"\b\d+(\.\d+)?%?\b", t))  # presence of stats/figures
    code_data = count_patterns(t, [r"\bcode\b", r"\breleased?\b", r"\bgithub\b", r"\bdataset(s)?\b", r"\bappendix\b"])
    s = 1 + min(2, methods > 0) + min(2, citations > 0) + min(1, numbers > 2) + min(1, code_data > 0)
    return max(1, min(5, s))

def score_selfrepair(t: str) -> int:
    # Self-Repair: audits, monitoring, incident reporting, rollback/pauses, independent review
    repair = count_patterns(t, [
        r"\btest(s|ing)?\b", r"\bverify|verification\b", r"\baudit(s|ing)?\b", r"\bmonitor(ing)?\b",
        r"\bincident reporting\b", r"\bpostmortem(s)?\b", r"\bfallback\b", r"\brollback\b",
        r"\bkill switch\b", r"\bpause\b", r"\bhalt\b", r"\boversight\b", r"\bindependent review\b",
        r"\bred team(ing)?\b", r"\brisk register\b", r"\bupdate(s|d)?\b"
    ])
    s = 1 + min(4, repair)
    return max(1, min(5, s))

def justify(name, s):
    tips = {
        'O': "State observation/uncertainty explicitly; avoid dogma.",
        'C': "Show premises → inference → conclusion; remove contradictions.",
        'E': "Prefer transparency/accountability; avoid manipulation.",
        'R': "Include methods, sources, figures, and code/data where possible.",
        'S': "Add audits/monitoring/incident reporting and rollback/pauses."
    }
    return f"{name}={s}/5 — {tips[name]}"

def render_report_md(base, ocers, notes, src_hash):
    lines = []
    lines.append(f"# OCERS Evaluation: {base}")
    lines.append("")
    lines.append(f"**Scores:** O={ocers['O']}, C={ocers['C']}, E={ocers['E']}, R={ocers['R']}, S={ocers['S']} → Total={ocers['total']}/25")
    lines.append(f"**Verdict:** {ocers['verdict']}")
    lines.append("")
    lines.append("**Notes:**")
    for k in ["O","C","E","R","S"]:
        lines.append(f"- {notes[k]}")
    lines.append("")
    lines.append(f"_Source SHA-256_: `{src_hash}`")
    lines.append("")
    return "\n".join(lines)

def main():
    if len(sys.argv) < 3:
        print("Usage: teof_ocers_min.py <input.txt> <outdir>")
        sys.exit(2)
    inpath = pathlib.Path(sys.argv[1]).resolve()
    outdir = pathlib.Path(sys.argv[2]).resolve()
    outdir.mkdir(parents=True, exist_ok=True)

    raw = read_text(str(inpath))
    text = norm_text(raw)

    O = score_observation(text)
    C = score_coherence(text)
    E = score_ethics(text)
    R = score_repro(text)
    S = score_selfrepair(text)
    total = O + C + E + R + S
    verdict = "PASS" if total >= 18 and min(O,C,E,R,S) >= 3 else "NEEDS-WORK"

    src_hash = hashlib.sha256(raw.encode("utf-8","ignore")).hexdigest()
    stamp = datetime.datetime.utcnow().isoformat()+"Z"
    base = inpath.stem

    out_json = {
        "stamp": stamp,
        "input_file": inpath.name,
        "hash_sha256": src_hash,
        "ocers": {"O":O,"C":C,"E":E,"R":R,"S":S,"total":total,"verdict":verdict},
        "notes": {
            "O": justify("O",O),
            "C": justify("C",C),
            "E": justify("E",E),
            "R": justify("R",R),
            "S": justify("S",S),
        }
    }
    with open(outdir/f"{base}.json","w",encoding="utf-8") as f:
        json.dump(out_json, f, ensure_ascii=False, indent=2)

    report_md = render_report_md(base, out_json["ocers"], out_json["notes"], src_hash)
    with open(outdir/f"{base}.report.md","w",encoding="utf-8") as f:
        f.write(report_md)

    print(f"[OCERS] {base}: total={total}/25 verdict={verdict}  (O={O} C={C} E={E} R={R} S={S})")
    print(f"→ wrote {outdir}/{base}.json and {outdir}/{base}.report.md")

if __name__ == "__main__":
    main()
